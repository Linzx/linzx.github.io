## 5.2 启动ServiceManager管家

Android系统有大量的服务， 例如Java层的ActivityManagerService, WindowManagerService服务， Native层的SurfaceFlinger, AudioFlinger服务等。Binder系统需要有一个统一的地方来管理这些服务， 对外提供服务注册、服务查询功能。

整个Binder系统中最先启动的是ServiceManager进程，ServiceManager作为Binder IPC的大管家，统管所有的Binder服务信息。 同时本身也是一个Binder服务，但并没有采用libbinder中的多线程模型来与Binder驱动通信，而是自行编写了binder.c直接和Binder驱动来通信。ServiceManager是单线程的进程， 不断地循环在binder_loop()过程来读取和处理事务，从而对外提供查询和注册服务的功能，这样的好处是简单而高效。

![此处增加一张Java/Native服务向ServiceManager注册或查询的关系图]()

> 介绍ServiceManager对外提供的功能： 注册和查询

> 查询国瓷， 要说明servicemanager对应的binder_node, handle=0 .


> 单例模式，

#### 1. 解析servicemanager.rc

ServiceManager是由init进程通过解析servicemanager.rc文件而创建的，如下：

```
service servicemanager /system/bin/servicemanager
    class core animation
    user system
    group system readproc
    critical
    onrestart restart healthd
    onrestart restart zygote
    onrestart restart audioserver
    onrestart restart media
    onrestart restart surfaceflinger
    onrestart restart inputflinger
    onrestart restart drm
    onrestart restart cameraserver
    writepid /dev/cpuset/system-background/tasks
```

init进程解析后，找到其所对应的可执行程序/system/bin/servicemanager，先来从servicemanager的启动过程说起。

    // service_manager.c
    int main(int argc, char **argv)
    {
        struct binder_state *bs;
        char *driver;
        if (argc > 1) {
            driver = argv[1];
        } else {
            driver = "/dev/binder"; // 默认的Binder设备节点
        }

        //Step 1: 打开binder驱动，申请128k字节内存
        bs = binder_open(driver, 128*1024);
        ...

        //Step 2: 成为上下文管理者
        if (binder_become_context_manager(bs)) {
            return -1;
        }
        ...

        //Step 3: 进入无限循环，处理client端发来的请求
        binder_loop(bs, svcmgr_handler);
        return 0;
    }

启动过程主要以下几个阶段：

- 首先，调用binder_open()方法来打开binder驱动，默认地采用/dev/binder设备节点，申请地内存空间大小为128KB；
- 其次，调用binder_become_context_manager()方法，将自己注册成为binder服务的大管家；
- 最后，调用binder_loop()方法进入无限循环，作为守护进程，随时待命等待处理client端发来的请求。

![create_servicemanager](/images/binder/create_servicemanager/create_servicemanager.jpg)


#### 2. 打开设备驱动

    // servicemanager/binder.c
    struct binder_state *binder_open(const char* driver, size_t mapsize)
    {
        struct binder_state *bs;
        bs = malloc(sizeof(*bs));

        // Step 1: 打开Binder设备驱动
        bs->fd = open(driver, O_RDWR);

        // Step 2: 验证binder版本是否一致
        if ((ioctl(bs->fd, BINDER_VERSION, &vers) == -1) ||
            (vers.protocol_version != BINDER_CURRENT_PROTOCOL_VERSION)) {
            goto fail_open;
        }

        // Step 3: 通过系统调用，mmap内存映射，mmap必须是page的整数倍
        bs->mapsize = mapsize;
        bs->mapped = mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs->fd, 0);
        return bs;

    fail_map:
        close(bs->fd);
    fail_open:
        free(bs);
        return NULL;
    }

> binder_open的过程，不管是malloc，open，mmap 一旦出现失败都是执行相应清理动作后直接退出， 比如结束，对于这些异常流程为了精简篇幅， 一般情况都会直接省略。

Linux设备驱动可以简单理解成一个文件， 调用open()以可读写方式打开/dev/binder设备节点， 经过系统调用陷入内核，然后由文件系统层层转换，最终进入Binder驱动会执行binder_open()。该方法主要工作是在Binder驱动层创建结构体binder_proc，再将binder_proc保存到fd->private_data，同时放入全局链表binder_procs。

通过ioctl系统调用向Binder驱动发送命令BINDER_VERSION，进入Binder驱动根据该命令则获取驱动层定义的BINDER_CURRENT_PROTOCOL_VERSION，
跟用户层同名变量进行比对， 看看binder协议版本是否一致， 目前版本只用于区别是32位或64位。

调用mmap()进行内存映射，映射的内存大小为128KB, mmap()方法经过系统调用对应于Binder驱动层的binder_mmap()，该方法主要工作是在Binder驱动层创建Binder_buffer对象，并放入当前binder_proc结构体的链表proc->buffers。

binder_open最核心的信息都记录在binder_state结构体， 打开dev/binder的文件描述符保存到bs->fd, mmap的内存地址保存到bs->mapped，对应内存大小保存到bs->mapsize。

> binder_open, binder_mmap, binder_ioctl都是Binder驱动层的知识点， 很多书籍要么一上来先讲Binder驱动层，要么干脆不讲，这都是不太合理的。驱动既然精华，又是难点。先让读者有一定概念，后续小节再进一步展开讲解。

#### 3. 注册成为大管家

    // servicemanager/binder.c
    int binder_become_context_manager(struct binder_state *bs)
    {
        return ioctl(bs->fd, BINDER_SET_CONTEXT_MGR, 0);
    }

通过ioctl系统调用向Binder驱动发送命令BINDER_SET_CONTEXT_MGR，成为上下文的管理者，由于servicemanager进程启动非常早，可以确定在Binder整体机制正式投入产线之前，就能完成向Binder驱动注册成为大管家的工作。 关于驱动层处理BINDER_SET_CONTEXT_MGR命令的主要任务：

- 保证每一个Binder上下文有且仅有一个binder管家实体，如果已存在则不再创建；
- 创建binder管家实体(binder_node)，初始化异步事务async_todo和binder_work两个队列，并分别增加其强弱引用计数。
- 初始化当前Binder_context的管家实体（binder_context_mgr_node）和管家uid(binder_context_mgr_uid)信息


> binder_context是Android 8.0新引入的结构体，用于支持Project Treble， 提供多个Binder域(上下文)，包括/dev/binder, /dev/hwbinder，/dev/vndbinder，具体每个Binder域含义后续再讲解。


#### 4. 等待客户请求

    // servicemanager/binder.c
    void binder_loop(struct binder_state *bs, binder_handler func)
    {
        int res;
        struct binder_write_read bwr;
        uint32_t readbuf[32];

        bwr.write_size = 0;
        bwr.write_consumed = 0;
        bwr.write_buffer = 0;

        readbuf[0] = BC_ENTER_LOOPER;
        //向binder驱动发送BC_ENTER_LOOPER协议
        binder_write(bs, readbuf, sizeof(uint32_t));

        for (;;) {
            bwr.read_size = sizeof(readbuf);
            bwr.read_consumed = 0;
            bwr.read_buffer = (uintptr_t) readbuf;
            //等待客户的数据
            res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
            //解析binder信息
            res = binder_parse(bs, 0, (uintptr_t) readbuf, bwr.read_consumed, func);
        }
    }

servicemanager先向Binder驱动发送BC_ENTER_LOOPER协议，让ServiceManager进入循环。然后再向驱动发送BINDER_WRITE_READ命令， 进入内核态，等待客户端的请求数据。若没有数据，则进入等待状态，直到收到数据后返回用户态，解析并处理，周而复始地不断循环该过程。

（1）先来看看ServiceManager进入循环前的动作

    int binder_write(struct binder_state *bs, void *data, size_t len)
    {
        struct binder_write_read bwr;
        int res;
        bwr.write_size = len; //此时len = 4
        bwr.write_consumed = 0;
        bwr.write_buffer = (uintptr_t) data; //此时data为BC_ENTER_LOOPER
        bwr.read_size = 0;
        bwr.read_consumed = 0;
        bwr.read_buffer = 0;
        res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
        return res;
    }

根据传递进来的参数初始化bwr，其中write_size大小为4，write_buffer指向缓冲区的起始地址，其内容为BC_ENTER_LOOPER请求协议号。通过ioctl将bwr数据发送给binder驱动。驱动收到BC_ENTER_LOOPER协议，主要工作是将servicemanager的当前线程的looper状态为BINDER_LOOPER_STATE_ENTERED(代表Binder主线程进入循环状态)。

（2） 再看看servicemanager解析过程

通过ioctl向Binder驱动发送BINDER_WRITE_READ命令，这是向驱动进行读与写操作的命令，当写缓存有数据则会执行写操作， 当读缓存有数据则会执行读操作。 如果读缓存没有没有数据时，则等待客户端发起请求， servicemanager一旦读取到事件，则会把数据保存到readbuf，然后交由binder_parse来解析。

    int binder_parse(struct binder_state *bs, struct binder_io *bio,
                     uintptr_t ptr, size_t size, binder_handler func)
    {
        int r = 1;
        uintptr_t end = ptr + (uintptr_t) size;
        while (ptr < end) {
            uint32_t cmd = *(uint32_t *) ptr; //指向readbuf
            ptr += sizeof(uint32_t);
            switch(cmd) {
            case BR_TRANSACTION: {
                struct binder_transaction_data *txn = (struct binder_transaction_data *) ptr;
                ...
                if (func) {  // 是指svcmgr_handler()函数
                    unsigned rdata[256/4];
                    struct binder_io msg;
                    struct binder_io reply;
                    int res;
                    bio_init(&reply, rdata, sizeof(rdata), 4); // 初始化binder_io结构体
                    bio_init_from_txn(&msg, txn); //将binder_transaction_data信息解析成binder_io
                    res = func(bs, txn, &msg, &reply);  // 真正处理业务逻辑的核心地方
                    if (txn->flags & TF_ONE_WAY) {
                        binder_free_buffer(bs, txn->data.ptr.buffer);
                    } else {
                        binder_send_reply(bs, &reply, txn->data.ptr.buffer, res);
                    }
                }
                ptr += sizeof(*txn);
                break;
            }
            case BR_REPLY: {
                struct binder_transaction_data *txn = (struct binder_transaction_data *) ptr;
                ...
                if (bio) {
                    bio_init_from_txn(bio, txn);
                    bio = 0;
                }
                ptr += sizeof(*txn);
                r = 0;
                break;
            }
            case BR_DEAD_BINDER: {
                struct binder_death *death = (struct binder_death *)(uintptr_t) *(binder_uintptr_t *)ptr;
                ptr += sizeof(binder_uintptr_t);
                death->func(bs, death->ptr); //处理binder死亡消息
                break;
            }
            default:
                return -1;
            }
        }
        return r;
    }

该过程协议主要：

- 第一类：BR_NOOP，BR_TRANSACTION_COMPLETE，BR_INCREFS，BR_ACQUIRE，BR_RELEASE，BR_DECREFS， 这些协议并没有什么工作需要处理；
- 第二类：BR_TRANSACTION，BR_REPLY，  接收到binder事务，这是最常用的协议
- 第二类：BR_DEAD_BINDER， 对端binder服务所在进程死亡后的通知处理
- 第四类：BR_FAILED_REPLY，BR_DEAD_REPLY，这些协议代表Binder通信过程出现异常



    int svcmgr_handler(struct binder_state *bs,
                       struct binder_transaction_data *txn,
                       struct binder_io *msg,
                       struct binder_io *reply)
    {
        struct svcinfo *si; //【见小节2.6.1】
        uint16_t *s;
        size_t len;
        uint32_t handle;
        uint32_t strict_policy;
        int allow_isolated;
        ...

        strict_policy = bio_get_uint32(msg);
        s = bio_get_string16(msg, &len);
        ...

        switch(txn->code) {
        case SVC_MGR_GET_SERVICE:
        case SVC_MGR_CHECK_SERVICE:
            s = bio_get_string16(msg, &len); //服务名
            //根据名称查找相应服务 【见小节3.1】
            handle = do_find_service(bs, s, len, txn->sender_euid, txn->sender_pid);
            //【见小节3.1.2】
            bio_put_ref(reply, handle);
            return 0;

        case SVC_MGR_ADD_SERVICE:
            s = bio_get_string16(msg, &len); //服务名
            handle = bio_get_ref(msg); //handle【见小节3.2.3】
            allow_isolated = bio_get_uint32(msg) ? 1 : 0;
             //注册指定服务 【见小节3.2】
            if (do_add_service(bs, s, len, handle, txn->sender_euid,
                allow_isolated, txn->sender_pid))
                return -1;
            break;

        case SVC_MGR_LIST_SERVICES: {
            uint32_t n = bio_get_uint32(msg);

            if (!svc_can_list(txn->sender_pid)) {
                return -1;
            }
            si = svclist;
            while ((n-- > 0) && si)
                si = si->next;
            if (si) {
                bio_put_string16(reply, si->name);
                return 0;
            }
            return -1;
        }
        default:
            return -1;
        }

        bio_put_uint32(reply, 0);
        return 0;
    }

该方法的功能：查询服务，注册服务，以及列举所有服务

【画一个图】

    struct svcinfo
    {
        struct svcinfo *next;
        uint32_t handle; //服务的handle值
        struct binder_death death;
        int allow_isolated;
        size_t len; //名字长度
        uint16_t name[0]; //服务名
    };

每一个服务用svcinfo结构体来表示，该handle值是在注册服务的过程中，由服务所在进程那一端所确定的。

## 三. 核心工作

servicemanager的核心工作就是注册服务和查询服务。

### 3.1 do_find_service
[-> service_manager.c]

    uint32_t do_find_service(struct binder_state *bs, const uint16_t *s, size_t len, uid_t uid, pid_t spid)
    {
        //查询相应的服务 【见小节3.1.1】
        struct svcinfo *si = find_svc(s, len);

        if (!si || !si->handle) {
            return 0;
        }

        if (!si->allow_isolated) {
            uid_t appid = uid % AID_USER;
            //检查该服务是否允许孤立于进程而单独存在
            if (appid >= AID_ISOLATED_START && appid <= AID_ISOLATED_END) {
                return 0;
            }
        }

        //服务是否满足查询条件
        if (!svc_can_find(s, len, spid)) {
            return 0;
        }
        return si->handle;
    }

查询到目标服务，并返回该服务所对应的handle

#### 3.1.1 find_svc

    struct svcinfo *find_svc(const uint16_t *s16, size_t len)
    {
        struct svcinfo *si;

        for (si = svclist; si; si = si->next) {
            //当名字完全一致，则返回查询到的结果
            if ((len == si->len) &&
                !memcmp(s16, si->name, len * sizeof(uint16_t))) {
                return si;
            }
        }
        return NULL;
    }

从svclist服务列表中，根据服务名遍历查找是否已经注册。当服务已存在`svclist`，则返回相应的服务名，否则返回NULL。

当找到服务的handle, 则调用bio_put_ref(reply, handle)，将handle封装到reply.

#### 3.1.2 bio_put_ref

    void bio_put_ref(struct binder_io *bio, uint32_t handle)
    {
        struct flat_binder_object *obj;

        if (handle)
            obj = bio_alloc_obj(bio); //[见小节3.1.3]
        else
            obj = bio_alloc(bio, sizeof(*obj));

        if (!obj)
            return;

        obj->flags = 0x7f | FLAT_BINDER_FLAG_ACCEPTS_FDS;
        obj->type = BINDER_TYPE_HANDLE; //返回的是HANDLE类型
        obj->handle = handle;
        obj->cookie = 0;
    }

#### 3.1.3 bio_alloc_obj

    static struct flat_binder_object *bio_alloc_obj(struct binder_io *bio)
    {
        struct flat_binder_object *obj;
        obj = bio_alloc(bio, sizeof(*obj));//[见小节3.1.4]

        if (obj && bio->offs_avail) {
            bio->offs_avail--;
            *bio->offs++ = ((char*) obj) - ((char*) bio->data0);
            return obj;
        }
        bio->flags |= BIO_F_OVERFLOW;
        return NULL;
    }

#### 3.1.4 bio_alloc
    static void *bio_alloc(struct binder_io *bio, size_t size)
    {
        size = (size + 3) & (~3);
        if (size > bio->data_avail) {
            bio->flags |= BIO_F_OVERFLOW;
            return NULL;
        } else {
            void *ptr = bio->data;
            bio->data += size;
            bio->data_avail -= size;
            return ptr;
        }
    }

### 3.2 do_add_service
[-> service_manager.c]

    int do_add_service(struct binder_state *bs,
                       const uint16_t *s, size_t len,
                       uint32_t handle, uid_t uid, int allow_isolated,
                       pid_t spid)
    {
        struct svcinfo *si;

        if (!handle || (len == 0) || (len > 127))
            return -1;

        //权限检查【见小节3.2.1】
        if (!svc_can_register(s, len, spid)) {
            return -1;
        }

        //服务检索【见小节3.1.1】
        si = find_svc(s, len);
        if (si) {
            if (si->handle) {
                svcinfo_death(bs, si); //服务已注册时，释放相应的服务【见小节3.2.2】
            }
            si->handle = handle;
        } else {
            si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));
            if (!si) {  //内存不足，无法分配足够内存
                return -1;
            }
            si->handle = handle;
            si->len = len;
            memcpy(si->name, s, (len + 1) * sizeof(uint16_t)); //内存拷贝服务信息
            si->name[len] = '\0';
            si->death.func = (void*) svcinfo_death;
            si->death.ptr = si;
            si->allow_isolated = allow_isolated;
            si->next = svclist; // svclist保存所有已注册的服务
            svclist = si;
        }

        //以BC_ACQUIRE命令，handle为目标的信息，通过ioctl发送给binder驱动
        binder_acquire(bs, handle);
        //以BC_REQUEST_DEATH_NOTIFICATION命令的信息，通过ioctl发送给binder驱动，主要用于清理内存等收尾工作。[见小节3.3]
        binder_link_to_death(bs, handle, &si->death);
        return 0;
    }

注册服务的分以下3部分工作：

- svc_can_register：检查权限，检查selinux权限是否满足；
- find_svc：服务检索，根据服务名来查询匹配的服务；
- svcinfo_death：释放服务，当查询到已存在同名的服务，则先清理该服务信息，再将当前的服务加入到服务列表svclist；

#### 3.2.1 svc_can_register
[-> service_manager.c]

    static int svc_can_register(const uint16_t *name, size_t name_len, pid_t spid)
    {
        const char *perm = "add";
        //检查selinux权限是否满足
        return check_mac_perms_from_lookup(spid, perm, str8(name, name_len)) ? 1 : 0;
    }

#### 3.2.2 svcinfo_death
[-> service_manager.c]

    void svcinfo_death(struct binder_state *bs, void *ptr)
    {
        struct svcinfo *si = (struct svcinfo* ) ptr;

        if (si->handle) {
            binder_release(bs, si->handle);
            si->handle = 0;
        }
    }

#### 3.2.3 bio_get_ref
[-> servicemanager/binder.c]

    uint32_t bio_get_ref(struct binder_io *bio)
    {
        struct flat_binder_object *obj;

        obj = _bio_get_obj(bio);
        if (!obj)
            return 0;

        if (obj->type == BINDER_TYPE_HANDLE)
            return obj->handle;

        return 0;
    }

### 3.3 binder_link_to_death
[-> servicemanager/binder.c]

    void binder_link_to_death(struct binder_state *bs, uint32_t target, struct binder_death *death)
    {
        struct {
            uint32_t cmd;
            struct binder_handle_cookie payload;
        } __attribute__((packed)) data;

        data.cmd = BC_REQUEST_DEATH_NOTIFICATION;
        data.payload.handle = target;
        data.payload.cookie = (uintptr_t) death;
        binder_write(bs, &data, sizeof(data)); //[见小节3.3.1]
    }

binder_write经过跟小节2.4.1一样的方式, 进入Binder driver后,直接调用后进入binder_thread_write, 处理BC_REQUEST_DEATH_NOTIFICATION命令


#### 3.3.1 binder_ioctl_write_read
[-> kernel/drivers/android/binder.c]

    static int binder_ioctl_write_read(struct file *filp,
                    unsigned int cmd, unsigned long arg,
                    struct binder_thread *thread)
    {
        int ret = 0;
        struct binder_proc *proc = filp->private_data;
        void __user *ubuf = (void __user *)arg;
        struct binder_write_read bwr;

        if (copy_from_user(&bwr, ubuf, sizeof(bwr))) { //把用户空间数据ubuf拷贝到bwr
            ret = -EFAULT;
            goto out;
        }
        if (bwr.write_size > 0) { //此时写缓存有数据【见小节3.3.2】
            ret = binder_thread_write(proc, thread,
                      bwr.write_buffer, bwr.write_size, &bwr.write_consumed);
             if (ret < 0) {
                  bwr.read_consumed = 0;
                  if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
                      ret = -EFAULT;
                  goto out;
              }
        }

        if (bwr.read_size > 0) { //此时读缓存有数据【见小节3.3.3】
            ret = binder_thread_read(proc, thread, bwr.read_buffer,
                     bwr.read_size,
                     &bwr.read_consumed,
                     filp->f_flags & O_NONBLOCK);
            if (!list_empty(&proc->todo))  //进程todo队列不为空,则唤醒该队列中的线程
                wake_up_interruptible(&proc->wait);
            if (ret < 0) {
                if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
                    ret = -EFAULT;
                goto out;
            }
        }

        if (copy_to_user(ubuf, &bwr, sizeof(bwr))) { //将内核数据bwr拷贝到用户空间ubuf
            ret = -EFAULT;
            goto out;
        }
    out:
        return ret;
    }

#### 3.3.2 binder_thread_write
[-> kernel/drivers/android/binder.c]

    static int binder_thread_write(struct binder_proc *proc,
          struct binder_thread *thread,
          binder_uintptr_t binder_buffer, size_t size,
          binder_size_t *consumed)
    {
      uint32_t cmd;
      struct binder_context *context = proc->context;
      void __user *buffer = (void __user *)(uintptr_t)binder_buffer;
      void __user *ptr = buffer + *consumed; //ptr指向小节3.2.3中bwr中write_buffer的data.
      void __user *end = buffer + size;
      while (ptr < end && thread->return_error == BR_OK) {
        get_user(cmd, (uint32_t __user *)ptr); //获取BC_REQUEST_DEATH_NOTIFICATION
        ptr += sizeof(uint32_t);
        switch (cmd) {
            case BC_REQUEST_DEATH_NOTIFICATION:{ //注册死亡通知
                uint32_t target;
                void __user *cookie;
                struct binder_ref *ref;
                struct binder_ref_death *death;

                get_user(target, (uint32_t __user *)ptr); //获取target
                ptr += sizeof(uint32_t);
                get_user(cookie, (void __user * __user *)ptr); //获取death
                ptr += sizeof(void *);

                ref = binder_get_ref(proc, target); //拿到目标服务的binder_ref

                if (cmd == BC_REQUEST_DEATH_NOTIFICATION) {
                    if (ref->death) {
                        break;  //已设置死亡通知
                    }
                    death = kzalloc(sizeof(*death), GFP_KERNEL);

                    INIT_LIST_HEAD(&death->work.entry);
                    death->cookie = cookie;
                    ref->death = death;
                    if (ref->node->proc == NULL) { //当目标binder服务所在进程已死,则发送死亡通知
                        ref->death->work.type = BINDER_WORK_DEAD_BINDER;
                        //当前线程为binder线程,则直接添加到当前线程的todo队列. 接下来,进入[小节3.2.6]
                        if (thread->looper & (BINDER_LOOPER_STATE_REGISTERED | BINDER_LOOPER_STATE_ENTERED)) {
                            list_add_tail(&ref->death->work.entry, &thread->todo);
                        } else {
                            list_add_tail(&ref->death->work.entry, &proc->todo);
                            wake_up_interruptible(&proc->wait);
                        }
                    }
                } else {
                    ...
                }
            } break;
          case ...;
        }
        *consumed = ptr - buffer;
      }
   }

此方法中的proc, thread都是指当前servicemanager进程的信息. 此时TODO队列有数据,则进入binder_thread_read.

那么哪些场景会向队列增加BINDER_WORK_DEAD_BINDER事务呢? 那就是当binder所在进程死亡后,会调用binder_release方法,
然后调用binder_node_release.这个过程便会发出死亡通知的回调.

#### 3.3.3 binder_thread_read

    static int binder_thread_read(struct binder_proc *proc,
                      struct binder_thread *thread,
                      binder_uintptr_t binder_buffer, size_t size,
                      binder_size_t *consumed, int non_block)
        ...
        //只有当前线程todo队列为空，并且transaction_stack也为空，才会开始处于当前进程的事务
        if (wait_for_proc_work) {
            ...
            ret = wait_event_freezable_exclusive(proc->wait, binder_has_proc_work(proc, thread));
        } else {
            ...
            ret = wait_event_freezable(thread->wait, binder_has_thread_work(thread));
        }
        binder_lock(__func__); //加锁

        if (wait_for_proc_work)
            proc->ready_threads--; //空闲的binder线程减1
        thread->looper &= ~BINDER_LOOPER_STATE_WAITING;

        while (1) {
            uint32_t cmd;
            struct binder_transaction_data tr;
            struct binder_work *w;
            struct binder_transaction *t = NULL;

            //从todo队列拿出前面放入的binder_work, 此时type为BINDER_WORK_DEAD_BINDER
            if (!list_empty(&thread->todo)) {
                w = list_first_entry(&thread->todo, struct binder_work,
                             entry);
            } else if (!list_empty(&proc->todo) && wait_for_proc_work) {
                w = list_first_entry(&proc->todo, struct binder_work,
                             entry);
            }

            switch (w->type) {
                case BINDER_WORK_DEAD_BINDER: {
                  struct binder_ref_death *death;
                  uint32_t cmd;

                  death = container_of(w, struct binder_ref_death, work);
                  if (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION)
                      ...
                  else
                      cmd = BR_DEAD_BINDER; //进入此分支
                  put_user(cmd, (uint32_t __user *)ptr);//拷贝到用户空间[见小节3.3.4]
                  ptr += sizeof(uint32_t);

                  //此处的cookie是前面传递的svcinfo_death
                  put_user(death->cookie, (binder_uintptr_t __user *)ptr);
                  ptr += sizeof(binder_uintptr_t);

                  if (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION) {
                      ...
                  } else
                      list_move(&w->entry, &proc->delivered_death);
                  if (cmd == BR_DEAD_BINDER)
                      goto done;
                } break;
            }
        }
        ...
        return 0;
    }

将命令BR_DEAD_BINDER写到用户空间, 此处的cookie是前面传递的svcinfo_death. 当binder_loop下一次
执行binder_parse的过程便会处理该消息。

#### 3.3.4 binder_parse
[-> servicemanager/binder.c]

    int binder_parse(struct binder_state *bs, struct binder_io *bio,
                     uintptr_t ptr, size_t size, binder_handler func)
    {
        int r = 1;
        uintptr_t end = ptr + (uintptr_t) size;

        while (ptr < end) {
            uint32_t cmd = *(uint32_t *) ptr;
            ptr += sizeof(uint32_t);
            switch(cmd) {
                case BR_DEAD_BINDER: {
                    struct binder_death *death = (struct binder_death *)(uintptr_t) *(binder_uintptr_t *)ptr;
                    ptr += sizeof(binder_uintptr_t);
                    // binder死亡消息【见小节3.3.5】
                    death->func(bs, death->ptr);
                    break;
                }
                ...
            }
        }
        return r;
    }

由小节3.2的 si->death.func = (void*) svcinfo_death; 可知此处 death->func便是执行svcinfo_death()方法.

#### 3.3.5 svcinfo_death
[-> service_manager.c]

    void svcinfo_death(struct binder_state *bs, void *ptr)
    {
        struct svcinfo *si = (struct svcinfo* ) ptr;

        if (si->handle) {
            binder_release(bs, si->handle);
            si->handle = 0;
        }
    }

#### 3.3.6 binder_release
[-> service_manager.c]

    void binder_release(struct binder_state *bs, uint32_t target)
    {
        uint32_t cmd[2];
        cmd[0] = BC_RELEASE;
        cmd[1] = target;
        binder_write(bs, cmd, sizeof(cmd));
    }

向Binder Driver写入BC_RELEASE命令, 最终进入Binder Driver后执行binder_dec_ref(ref, 1)来减少binder node的引用.

### 3.4 binder_send_reply
[-> servicemanager/binder.c]

    void binder_send_reply(struct binder_state *bs,
                           struct binder_io *reply,
                           binder_uintptr_t buffer_to_free,
                           int status)
    {
        struct {
            uint32_t cmd_free;
            binder_uintptr_t buffer;
            uint32_t cmd_reply;
            struct binder_transaction_data txn;
        } __attribute__((packed)) data;

        data.cmd_free = BC_FREE_BUFFER; //free buffer命令
        data.buffer = buffer_to_free;
        data.cmd_reply = BC_REPLY; // reply命令
        data.txn.target.ptr = 0;
        data.txn.cookie = 0;
        data.txn.code = 0;
        if (status) {
            data.txn.flags = TF_STATUS_CODE;
            data.txn.data_size = sizeof(int);
            data.txn.offsets_size = 0;
            data.txn.data.ptr.buffer = (uintptr_t)&status;
            data.txn.data.ptr.offsets = 0;
        } else {
            data.txn.flags = 0;
            data.txn.data_size = reply->data - reply->data0;
            data.txn.offsets_size = ((char*) reply->offs) - ((char*) reply->offs0);
            data.txn.data.ptr.buffer = (uintptr_t)reply->data0;
            data.txn.data.ptr.offsets = (uintptr_t)reply->offs0;
        }
        //向Binder驱动通信
        binder_write(bs, &data, sizeof(data));
    }

当小节2.5执行binder_parse方法，先调用svcmgr_handler()，再然后执行binder_send_reply过程。该方法会调用
[小节2.4.1] binder_write进入binder驱动后，将BC_FREE_BUFFER和BC_REPLY命令协议发送给Binder驱动，向client端发送reply.
其中data的数据区中保存的是TYPE为HANDLE.

## 四. 总结

ServiceManger集中管理系统内的所有服务，通过权限控制进程是否有权注册服务,通过字符串名称来查找对应的Service;
由于ServiceManger进程建立跟所有向其注册服务的死亡通知, 那么当服务所在进程死亡后, 会只需告知ServiceManager.
每个Client通过查询ServiceManager可获取Server进程的情况，降低所有Client进程直接检测会导致负载过重。

**ServiceManager启动流程：**

1. 打开binder驱动，并调用mmap()方法分配128k的内存映射空间：binder_open();
2. 通知binder驱动使其成为守护进程：binder_become_context_manager()；
3. 验证selinux权限，判断进程是否有权注册或查看指定服务；
4. 进入循环状态，等待Client端的请求：binder_loop()。
5. 注册服务的过程，根据服务名称，但同一个服务已注册，重新注册前会先移除之前的注册信息；
5. 死亡通知: 当binder所在进程死亡后,会调用binder_release方法,然后调用binder_node_release.这个过程便会发出死亡通知的回调.

ServiceManager最核心的两个功能为查询和注册服务：

- 注册服务：记录服务名和handle信息，保存到svclist列表；
- 查询服务：根据服务名查询相应的的handle信息。



###################


open 对应 binder_open 后续解析
mmap
ioctl --> binder_ioctl_write_read -> binder_thread_read



- proc->outer_lock， 数据类型为spinlock_t,  binder_proc_lock/binder_proc_unlock，用于保护binder_ref
- proc->inner_lock， 数据类型为spinlock_t， binder_inner_proc_lock/binder_inner_proc_unlock，
用于保护线程和节点列表(proc->threads, proc->waiting_threads, proc->nodes), 和
binder_proc相关的所有todo列表(proc->todo, thread->todo, proc->delivered_death，node->async_todo)，
以及thread->transaction_stack
- node->lock， 数据类型为spinlock_t， binder_node_lock/binder_node_unlock， 用于保护大多数的binder_node

- binder_transaction->lock， 数据类型为spinlock_t
- binder_context->context_mgr_node_lock， 数据类型为struct mutex

A进程下的任何锁都不能嵌套在B进程的同一级别或更低的任何锁上。


- foo_olocked() : 需要 proc->outer_lock
- foo_ilocked() : 需要 proc->inner_lock
- foo_nlocked() : 需要 node->lock
- foo_oilocked(): 需要 proc->outer_lock and proc->inner_lock
- foo_nilocked(): 需要 node->lock and proc->inner_lock
